{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with NLTK\n",
    "In this tutorial, we will discuss how to use the Natural Language ToolKit (NLTK) for various NLP tasks. We will learn about different NLP task such as\n",
    "tokenization, stemming, lemmatization, stop word removal, POS tagging, chunking, named entity recognition and some basics about the WordNet Interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the NLTK library and downloading relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eshban/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eshban/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/eshban/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/eshban/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/eshban/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/eshban/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Will be used in PunktTokenizer\n",
    "nltk.download('wordnet') # Will be used in WordNet basics\n",
    "nltk.download('averaged_perceptron_tagger') # Will be used for POS Tagging\n",
    "nltk.download('stopwords') # Will be used in stop words removal\n",
    "nltk.download('maxent_ne_chunker') # Will be used for NE-Chunking\n",
    "nltk.download('words') # Will be used somewhere :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization\n",
    "Sentence tokenization is the process of tokenizing a text into sentences. To perform sentence-level tokenization, NLTK provides a method called <code>sent_tokenize</code>. This method uses an instance of <code>PunktSentenceTokenizer.</code>\n",
    "<br>\n",
    "We import the <code>sent_tokenize</code> method as depicted in the code snippet below. The method takes a string as a parameter and returns an array of sentences. The tokenizer is already trained for English and a few other European languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Success is not final. Failure is not fatal. It is the courage to continue that counts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Success is not final.', 'Failure is not fatal.', 'It is the courage to continue that counts.']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success is not final.\n",
      "Failure is not fatal.\n",
      "It is the courage to continue that counts.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence_tokens:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization\n",
    "Word tokenization is the process of tokenizing sentences or text into words and punctuation. NLTK provides several ways to perform word-level tokenization. <br>\n",
    "It provides a method called <code>word_tokenize</code>, which splits text using punctuation and non-alphabetic characters. This method is a wrapper method for the <code>TreebankWordTokenizer.</code> Therefore, the result from both are identical.<br>\n",
    "NLTK also provides other tokenizers, such as <code>WordPunctTokenizer</code> and <code>WhitespaceTokenizer.</code> <code>WordPunctTokenizer</code> also splits the text from the punctuation. But unlike the <code>TreebankWordTokenizer</code>, this tokenizer splits the punctuation into separate tokens. <code>WhitespaceTokenizer</code>, as the name suggests, splits the text using white spaces. There are a few other tokenizers available as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Let's see how the tokenizer splits this!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'see', 'how', 'the', 'tokenizer', 'splits', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer, WhitespaceTokenizer\n",
    "\n",
    "tree_tokenizer = TreebankWordTokenizer()\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "white_space_tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'s\", 'see', 'how', 'the', 'tokenizer', 'splits', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tree_tokenizer.tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'see', 'how', 'the', 'tokenizer', 'splits', 'this!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = white_space_tokenizer.tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', \"'\", 's', 'see', 'how', 'the', 'tokenizer', 'splits', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_punct_tokenizer.tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is a heuristic process in which a word’s endings are chopped off in hope of achieving its base form. Stemming acts on words without knowing the context. Therefore, it’s faster but doesn’t always yield the desired result.<br>\n",
    "Stemming isn’t as easy as we presume. If it was, there would be only one implementation. Sadly, stemming is an imprecise science, which leads to issues such as <b>understemming</b> and <b>overstemming.</b><br>\n",
    "<i>Understemming</i> is the failure to reduce words with the same meaning to the same root. For example, <code>jumped</code> and <code>jumps</code> may be reduced to <code>jump</code>, while <code>jumpiness</code> may be reduced to <code>jumpi.</code><br>\n",
    "<i>Overstemming</i> is the failure to keep two words with distinct meanings separate. For instance, <code>general</code> and <code>generate</code> may both be stemmed to <code>gener.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n",
      "lie\n",
      "lie\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('lying'))\n",
    "print(porter_stemmer.stem('lies'))\n",
    "print(porter_stemmer.stem('lied'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n",
      "lie\n",
      "lied\n"
     ]
    }
   ],
   "source": [
    "lancaster_stemmer = LancasterStemmer()\n",
    "print(lancaster_stemmer.stem('lying'))\n",
    "print(lancaster_stemmer.stem('lies'))\n",
    "print(lancaster_stemmer.stem('lied'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n",
      "lie\n",
      "lie\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "print(snowball_stemmer.stem('lying'))\n",
    "print(snowball_stemmer.stem('lies'))\n",
    "print(snowball_stemmer.stem('lied'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "Lemmatization is a process that uses vocabulary and morphological analysis of words to remove the inflected endings to achieve its base form (dictionary form), which is known as the <i>lemma.</i><br>\n",
    "It’s a much more complicated and expensive process that requires an understanding of the context in which words appear in order to make decisions about what they mean. Hence, it uses a lexical vocabulary to derive the root form, is more time consuming than stemming, and is most likely to yield accurate results.<br>\n",
    "Lemmatization can be done with NLTK using <code>WordNetLemmatizer</code>, which uses a lexical database called <b>WordNet.</b><br>\n",
    "When using the <code>WordNetLemmatizer</code>, we should specify which <b>part of speech</b> should be used in order to derive the accurate lemma. Words can be in the form of Noun(n), Adjective(a), Verb(v), or Adverb(r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(\"Verb form: \" + lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "    print(\"Noun form: \" + lemmatizer.lemmatize(word, pos=\"n\"))\n",
    "    print(\"Adverb form: \" + lemmatizer.lemmatize(word, pos=\"r\"))\n",
    "    print(\"Adjective form: \" + lemmatizer.lemmatize(word, pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb form: ears\n",
      "Noun form: ear\n",
      "Adverb form: ears\n",
      "Adjective form: ears\n"
     ]
    }
   ],
   "source": [
    "lemmatize(\"ears\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb form: run\n",
      "Noun form: running\n",
      "Adverb form: running\n",
      "Adjective form: running\n"
     ]
    }
   ],
   "source": [
    "lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming VS Lemmatization\n",
    "Usage of either stemming or lemmatization will mostly depend on the situation at hand. If speed is required, it’s better to resort to stemming. But if accuracy is required it’s best to use lemmatization.<br>\n",
    "A simple example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer();\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deactiv\n",
      "deactiv\n",
      "deactiv\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"deactivating\"))\n",
    "print(stemmer.stem(\"deactivated\"))\n",
    "print(stemmer.stem(\"deactivates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deactivate\n",
      "deactivate\n",
      "deactivate\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"deactivating\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"deactivating\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"deactivating\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speak\n",
      "bedroom\n",
      "joke\n",
      "lisa\n",
      "purpl\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('stones')) \n",
    "print(stemmer.stem('speaking')) \n",
    "print(stemmer.stem('bedroom')) \n",
    "print(stemmer.stem('jokes')) \n",
    "print(stemmer.stem('lisa')) \n",
    "print(stemmer.stem('purple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speaking\n",
      "bedroom\n",
      "joke\n",
      "lisa\n",
      "purple\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('stones')) \n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    "print(lemmatizer.lemmatize('purple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) Tagging\n",
    "Part-Of-Speech tagging (or POS tagging) is also a very import component of NLP. The purpose of the POS tagging is to assign labels for each token (a word in this case) with its respective grammatical component, such as noun, verb, adjective, or adverb. Most POS are divided into sub-classes.\n",
    "<br>\n",
    "POS tagging can be identified as a supervised machine learning solution, mainly because it takes features like the previous word, next word, and capitalization of the first word into consideration when assigning a POS tag to a word.\n",
    "<br>\n",
    "The most popular tag set for POS tagging is <b>Penn Treebank tagset.</b> Most of the trained POS taggers for English are trained on this tag set. The following link shows the available POS Tags in <b>Penn Treebank tagset.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The hardest choices requires the strongest wills\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'hardest', 'choices', 'requires', 'the', 'strongest', 'wills']\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = word_tokenize(sentence)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('hardest', 'JJS'),\n",
       " ('choices', 'NNS'),\n",
       " ('requires', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('strongest', 'JJS'),\n",
       " ('wills', 'NNS')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "Chunking or shallow parsing is a process that extracts phrases from a text sample. Here we extract chunks of sentences that constitute meaning rather than identifying the sentence’s structure. This is different and more advanced than tokenization because it extracts phrases instead of tokens.<br>\n",
    "As an example, the word “North America” can be extracted as a single phrase using chunking rather than two separate words “North” and “America” as tokenization does.\n",
    "<br>\n",
    "Chunking is a process that requires POS tagged input, and it provides chunks of phrases as output. Same as in POS tags, there is a standard set of chunk tags like Noun Phrase(NP), Verb Phrase (VP), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the big vicious dog barked at the small feeble cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer = ('''NP: {<DT>?<JJ>*<NN>} # NP''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('vicious', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('barked', 'VBD'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('small', 'JJ'),\n",
       " ('feeble', 'JJ'),\n",
       " ('cat', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkParser = RegexpParser(grammer)\n",
    "tagged = pos_tag(word_tokenize(sentence))\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = chunkParser.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT big/JJ vicious/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT small/JJ feeble/JJ cat/NN))\n",
      "(NP the/DT big/JJ vicious/JJ dog/NN)\n",
      "(NP the/DT small/JJ feeble/JJ cat/NN)\n"
     ]
    }
   ],
   "source": [
    "for subtree in tree.subtrees():\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Removal\n",
    "<b>Stop words</b> are simply words that have very little meaning and are mostly used as part of the grammatical structure of a sentence. Words like “the”, “a”, “an”, “in”, etc. are considered stop-words.\n",
    "<br>\n",
    "Even though it doesn’t seem like much, stop word removal plays an important role when dealing with tasks such as sentiment analysis. This process is also used by search engines when indexing entries of a search query.\n",
    "<br>\n",
    "NLTK comes with the corpora <code>stopwords</code> which contains stop word lists for 16 different languages. No direct function is given by NLTK to remove stop words, but we can use the list to programmatically remove them from sentences.\n",
    "<br>\n",
    "If we are dealing with many sentences, first the text must be split into sentences using <code>sent_tokenize.</code> Then using <code>word_tokenize</code>, we can further break the sentences into words, and then remove the stop words using the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Success is not final. Failure is not fatal. It is the courage to continue that counts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Success', 'is', 'not', 'final', '.', 'Failure', 'is', 'not', 'fatal', '.', 'It', 'is', 'the', 'courage', 'to', 'continue', 'that', 'counts', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(sentence)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = word_tokens[:]\n",
    "for token in word_tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        clean_tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Success', 'final', '.', 'Failure', 'fatal', '.', 'It', 'courage', 'continue', 'counts', '.']\n"
     ]
    }
   ],
   "source": [
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "<b>Named entity recognition (NER)</b>, is the process of identifying entities such as Names, Locations, Dates, or Organizations that exist in an unstructured text sample.\n",
    "<br>\n",
    "The purpose of NER is to be able to map the extracted entities against a knowledge base, or to extract relationships between different entities. Eg: Who did what? or Where something take place? or At what time something occur?\n",
    "<br>\n",
    "For domain-specific entities, in a field like medicine or law, we’ll need to train our own NER algorithm.\n",
    "<br>\n",
    "For casual use, NLTK provides us with a method called <code>ne_chunk</code> to perform NER on a given text. In order to use <code>ne_chunk</code>, the text needs to first be tokenized into words and then POS tagged. After NER, the tagged words depict their respective entity type. In this case, <i>Mark</i> and <i>John</i> are of type <i>PERSON</i>, <i>Google</i> and <i>Yahoo</i> are of type <i>ORGANIZATION</i>, and <i>New York City</i> is of type <i>GPE</i> (which indicates location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Mark who works at Yahoo and John who works at Google decided to meet at New York City\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  who/WP\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Yahoo/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  who/WP\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  decided/VBD\n",
      "  to/TO\n",
      "  meet/VB\n",
      "  at/IN\n",
      "  (GPE New/NNP York/NNP City/NNP))\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Avengers began as a group of extraordinary individuals who were assembled to defeat \\\n",
    "Loki and his chitauri army in New York City. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION Avengers/NNP)\n",
      "  began/VBD\n",
      "  as/IN\n",
      "  a/DT\n",
      "  group/NN\n",
      "  of/IN\n",
      "  extraordinary/JJ\n",
      "  individuals/NNS\n",
      "  who/WP\n",
      "  were/VBD\n",
      "  assembled/VBN\n",
      "  to/TO\n",
      "  defeat/VB\n",
      "  (PERSON Loki/NNP)\n",
      "  and/CC\n",
      "  his/PRP$\n",
      "  chitauri/NN\n",
      "  army/NN\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP City/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Interface\n",
    "WordNet is a large English lexical database. Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.\n",
    "<br>\n",
    "<b>Synset or “synonym set” is a collection of synonymous words.</b>\n",
    "<br>\n",
    "NLTK provides an interface for the NLTK database, and it comes with the corpora module. WordNet is composed of approximately 155,200 words and 117,600 synonym sets that are logically related to each other.\n",
    "<br>\n",
    "As an example, in WordNet, a word like <b><i>computer</i></b> has two possible contexts (one being a machine for performing computation, and the other being a calculator: which is associated to computer in a lexical sense). It is identified by <code>computer.n.01</code> (is known as the \"lemma code name\". And letter <code>n</code> depicts that the word is a noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer.n.01'), Synset('calculator.n.01')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer',\n",
       " 'computing_machine',\n",
       " 'computing_device',\n",
       " 'data_processor',\n",
       " 'electronic_computer',\n",
       " 'information_processing_system']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wordnet.synset('computer.n.01')\n",
    "syn.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a machine for performing calculations automatically'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['large', 'large', 'big', 'large', 'bombastic', 'declamatory', 'large', 'orotund', 'tumid', 'turgid', 'big', 'large', 'magnanimous', 'big', 'large', 'prominent', 'large', 'big', 'enceinte', 'expectant', 'gravid', 'great', 'large', 'heavy', 'with_child', 'large', 'large', 'boastfully', 'vauntingly', 'big', 'large']\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('large'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small', 'little']\n"
     ]
    }
   ],
   "source": [
    "antonyms = []\n",
    "for syn in wordnet.synsets('large'):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
